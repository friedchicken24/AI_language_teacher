import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import google.generativeai as genai
import assemblyai as aai
import os
from gtts import gTTS
import av
import numpy as np
import uuid
import queue
from io import BytesIO
import wave
import librosa # Th√™m th∆∞ vi·ªán n√†y

# --- C·∫§U H√åNH BAN ƒê·∫¶U ---
st.set_page_config(page_title="Tr·ª£ l√Ω ·∫£o", page_icon="ü§ñ", layout="wide")
st.title("ü§ñ Tr·ª£ L√Ω ·∫¢o Th√¥ng Minh")

# --- C·∫§U H√åNH API KEYS V√Ä CLIENTS ---
try:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    aai.settings.api_key = st.secrets["ASSEMBLYAI_API_KEY"]
except KeyError as e:
    st.error(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y API Key: {e}. Vui l√≤ng ki·ªÉm tra l·∫°i Secrets.")
    st.stop()

# --- C√ÅC H√ÄM TI·ªÜN √çCH ---
# (C√°c h√†m get_ai_response v√† text_to_speech gi·ªØ nguy√™n)
def text_to_speech(text, lang='vi'):
    try:
        tts = gTTS(text=text, lang=lang, slow=False)
        filename = f"response_{uuid.uuid4()}.mp3"
        tts.save(filename)
        return filename
    except Exception as e:
        st.error(f"L·ªói khi t·∫°o gi·ªçng n√≥i: {e}")
        return None

def speech_to_text(wav_bytes):
    try:
        config = aai.TranscriptionConfig(language_code="vi")
        transcriber = aai.Transcriber(config=config)
        transcript = transcriber.transcribe(wav_bytes)

        if transcript.status == aai.TranscriptStatus.error:
            st.error(f"L·ªói t·ª´ AssemblyAI: {transcript.error}")
            return None
        return transcript.text
    except Exception as e:
        st.error(f"L·ªói khi nh·∫≠n di·ªán gi·ªçng n√≥i qua AssemblyAI: {e}")
        return None

def get_ai_response(user_text, conversation_history, system_prompt):
    model = genai.GenerativeModel('gemini-1.5-flash-latest')
    messages = [{"role": "system", "content": system_prompt}]
    gemini_history = []
    for entry in conversation_history:
        role = 'model' if entry['role'] == 'assistant' else entry['role']
        gemini_history.append({'role': role, 'parts': [entry['content']]})
    chat = model.start_chat(history=gemini_history)
    try:
        response = chat.send_message(user_text)
        return response.text
    except Exception as e:
        st.error(f"L·ªói khi g·ªçi Gemini: {e}")
        return "T√¥i xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë v·ªõi b·ªô n√£o c·ªßa m√¨nh."


# --- GIAO DI·ªÜN V√Ä LOGIC ---
with st.sidebar:
    st.title("T√πy Ch·ªçn Tr·ª£ L√Ω ·∫¢o")
    personas = {
        "Tr·ª£ l√Ω th√¢n thi·ªán": "B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o t√™n l√† Zen, r·∫•t th√¢n thi·ªán, t√≠ch c·ª±c v√† lu√¥n s·∫µn l√≤ng gi√∫p ƒë·ª°. H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.",
        "Nh√† s·ª≠ h·ªçc uy√™n b√°c": "B·∫°n l√† m·ªôt nh√† s·ª≠ h·ªçc uy√™n b√°c. H√£y tr·∫£ l·ªùi m·ªçi c√¢u h·ªèi v·ªõi gi·ªçng ƒëi·ªáu trang tr·ªçng, ƒë∆∞a ra c√°c chi ti·∫øt v√† b·ªëi c·∫£nh l·ªãch s·ª≠ th√∫ v·ªã. H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.",
        "Chuy√™n gia c√¥ng ngh·ªá": "B·∫°n l√† m·ªôt chuy√™n gia c√¥ng ngh·ªá h√†ng ƒë·∫ßu. H√£y gi·∫£i th√≠ch c√°c kh√°i ni·ªám ph·ª©c t·∫°p m·ªôt c√°ch ƒë∆°n gi·∫£n, ƒë∆∞a ra c√°c v√≠ d·ª• th·ª±c t·∫ø v√† lu√¥n c·∫≠p nh·∫≠t c√°c xu h∆∞·ªõng m·ªõi nh·∫•t. H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát."
    }
    selected_persona_name = st.selectbox("Ch·ªçn m·ªôt vai tr√≤:", options=list(personas.keys()))
    system_prompt = personas[selected_persona_name]

# Kh·ªüi t·∫°o session state
if "conversation" not in st.session_state:
    st.session_state.conversation = []
if "last_response_audio" not in st.session_state:
    st.session_state.last_response_audio = None
if "audio_buffer" not in st.session_state:
    st.session_state.audio_buffer = queue.Queue()

# Hi·ªÉn th·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán
chat_container = st.container()
with chat_container:
    for entry in st.session_state.conversation:
        with st.chat_message(entry["role"]):
            st.write(entry["content"])

# T·ª± ƒë·ªông ph√°t √¢m thanh
if st.session_state.last_response_audio:
    st.audio(st.session_state.last_response_audio, autoplay=True)
    if os.path.exists(st.session_state.last_response_audio):
        os.remove(st.session_state.last_response_audio)
    st.session_state.last_response_audio = None

# Component ghi √¢m
st.write("---")
col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("B·∫£ng ƒëi·ªÅu khi·ªÉn")
    def audio_frame_callback(frame: av.AudioFrame):
        st.session_state.audio_buffer.put(frame.to_ndarray())

    webrtc_ctx = webrtc_streamer(
        key="recorder", # ƒê√£ s·ª≠a l·∫°i key
        mode=WebRtcMode.SENDONLY,
        audio_frame_callback=audio_frame_callback,
        media_stream_constraints={"video": False, "audio": True},
        rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
    )

with col2:
    st.subheader("Tr·∫°ng th√°i")
    if webrtc_ctx.state.playing:
        st.success("üî¥ Micro ƒëang b·∫≠t. H√£y n√≥i ƒëi!")
        if st.button("D·ª´ng v√† g·ª≠i"):
            frames = []
            while not st.session_state.audio_buffer.empty():
                frames.append(st.session_state.audio_buffer.get())

            if not frames:
                st.warning("Kh√¥ng c√≥ √¢m thanh n√†o ƒë∆∞·ª£c ghi l·∫°i. Vui l√≤ng n√≥i g·∫ßn micro h∆°n.")
            else:
                st.info("ƒê√£ nh·∫≠n ƒë∆∞·ª£c √¢m thanh. ƒêang x·ª≠ l√Ω...")
                
                # --- PH·∫¶N RESAMPLE √ÇM THANH ---
                sound_chunk = np.concatenate(frames, axis=1).flatten()
                original_sr = 48000
                target_sr = 16000
                
                resampled_audio = librosa.resample(y=sound_chunk.astype(np.float32), orig_sr=original_sr, target_sr=target_sr)
                resampled_audio_int16 = (resampled_audio * 32767).astype(np.int16)
                
                wav_buffer = BytesIO()
                with wave.open(wav_buffer, "wb") as wf:
                    wf.setnchannels(1)
                    wf.setsampwidth(2)
                    wf.setframerate(target_sr) # D√πng t·∫ßn s·ªë m·ªõi
                    wf.writeframes(resampled_audio_int16.tobytes())
                
                wav_bytes = wav_buffer.getvalue()

                # ---- B·∫ÆT ƒê·∫¶U LU·ªíNG X·ª¨ L√ù ----
                with st.spinner("AI ƒëang l·∫Øng nghe..."):
                    user_text = speech_to_text(wav_bytes)

                if user_text:
                    st.session_state.conversation.append({"role": "user", "content": user_text})
                    with st.spinner("AI ƒëang suy nghƒ©..."):
                        ai_response_text = get_ai_response(user_text, st.session_state.conversation, system_prompt)
                    
                    st.session_state.conversation.append({"role": "assistant", "content": ai_response_text})
                    
                    with st.spinner("AI ƒëang chu·∫©n b·ªã n√≥i..."):
                        audio_file = text_to_speech(ai_response_text)
                    
                    if audio_file:
                        st.session_state.last_response_audio = audio_file
                    
                    st.rerun()
                else:
                    st.error("Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c gi·ªçng n√≥i. H√£y th·ª≠ n√≥i to v√† r√µ h∆°n.")
    else:
        st.info("Nh·∫•n 'Start' tr√™n khung ƒëen ƒë·ªÉ c·∫•p quy·ªÅn v√† b·∫Øt ƒë·∫ßu ghi √¢m.")
