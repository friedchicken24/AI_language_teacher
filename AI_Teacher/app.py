import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase
import google.generativeai as genai
import assemblyai as aai
import os
from gtts import gTTS
import av
import numpy as np
import uuid

# --- C·∫§U H√åNH BAN ƒê·∫¶U ---
st.set_page_config(page_title="Tr·ª£ l√Ω ·∫£o", page_icon="ü§ñ")
st.title("ü§ñ Tr·ª£ L√Ω ·∫¢o Th√¥ng Minh")
st.sidebar.title("T√πy Ch·ªçn Tr·ª£ L√Ω ·∫¢o")
st.sidebar.markdown("Ch·ªçn m·ªôt 'c√° t√≠nh' cho tr·ª£ l√Ω ·∫£o c·ªßa ch√∫ng ta!")

# L·∫•y API keys
try:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    aai.settings.api_key = st.secrets["ASSEMBLYAI_API_KEY"]
except KeyError as e:
    st.error(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y API Key: {e}. Vui l√≤ng ki·ªÉm tra l·∫°i Secrets.")
    st.stop()

# --- L·ªöP X·ª¨ L√ù √ÇM THANH ---
class AudioRecorder(AudioProcessorBase):
    def __init__(self):
        self.frames_buffer = []
    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        # Ch·ªâ c·∫ßn l∆∞u frame v√†o buffer
        self.frames_buffer.append(frame.to_ndarray())
        return frame

# --- C√ÅC H√ÄM TI·ªÜN √çCH ---
# (C√°c h√†m text_to_speech, speech_to_text, get_ai_response gi·ªØ nguy√™n nh∆∞ code c≈© c·ªßa b·∫°n)
# ... D√°n c√°c h√†m ƒë√≥ v√†o ƒë√¢y ...
def text_to_speech(text, lang='vi'):
    try:
        tts = gTTS(text=text, lang=lang, slow=False)
        filename = f"response_{uuid.uuid4()}.mp3"
        tts.save(filename)
        return filename
    except Exception as e:
        st.error(f"L·ªói khi t·∫°o gi·ªçng n√≥i: {e}")
        return None

def speech_to_text(audio_frames, sample_rate):
    try:
        sound_chunk = np.concatenate(audio_frames, axis=1)
        output_filename = f"input_{uuid.uuid4()}.wav"
        with open(output_filename, "wb") as f:
            f.write(b"RIFF")
            f.write(b"\x00\x00\x00\x00")
            f.write(b"WAVE")
            f.write(b"fmt ")
            f.write(b"\x10\x00\x00\x00")
            f.write(b"\x01\x00\x01\x00")
            f.write(sample_rate.to_bytes(4, "little"))
            f.write((sample_rate * 2).to_bytes(4, "little"))
            f.write(b"\x02\x00\x10\x00")
            f.write(b"data")
            f.write(len(sound_chunk.tobytes()).to_bytes(4, "little"))
            f.write(sound_chunk.tobytes())
        transcriber = aai.Transcriber()
        transcript = transcriber.transcribe(output_filename)
        os.remove(output_filename)
        if transcript.status == aai.TranscriptStatus.error:
            st.error(f"L·ªói t·ª´ AssemblyAI: {transcript.error}")
            return None
        else:
            return transcript.text
    except Exception as e:
        st.error(f"L·ªói khi nh·∫≠n di·ªán gi·ªçng n√≥i qua AssemblyAI: {e}")
        return None

def get_ai_response(user_text, conversation_history, system_prompt):
    model = genai.GenerativeModel('gemini-1.5-flash-latest')
    messages = [{"role": "system", "content": system_prompt}]
    gemini_history = []
    for entry in conversation_history:
        role = 'model' if entry['role'] == 'assistant' else entry['role']
        gemini_history.append({'role': role, 'parts': [entry['content']]})
    chat = model.start_chat(history=gemini_history)
    try:
        response = chat.send_message(user_text)
        return response.text
    except Exception as e:
        st.error(f"L·ªói khi g·ªçi Gemini: {e}")
        return "T√¥i xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë v·ªõi b·ªô n√£o c·ªßa m√¨nh."

# --- C√Å T√çNH C·ª¶A AI ---
personas = {
    "Tr·ª£ l√Ω th√¢n thi·ªán": "B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o t√™n l√† Zen, r·∫•t th√¢n thi·ªán, t√≠ch c·ª±c v√† lu√¥n s·∫µn l√≤ng gi√∫p ƒë·ª°. H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.",
    "Nh√† s·ª≠ h·ªçc uy√™n b√°c": "B·∫°n l√† m·ªôt nh√† s·ª≠ h·ªçc uy√™n b√°c. H√£y tr·∫£ l·ªùi m·ªçi c√¢u h·ªèi v·ªõi gi·ªçng ƒëi·ªáu trang tr·ªçng, ƒë∆∞a ra c√°c chi ti·∫øt v√† b·ªëi c·∫£nh l·ªãch s·ª≠ th√∫ v·ªã. H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.",
    "Chuy√™n gia c√¥ng ngh·ªá": "B·∫°n l√† m·ªôt chuy√™n gia c√¥ng ngh·ªá h√†ng ƒë·∫ßu. H√£y gi·∫£i th√≠ch c√°c kh√°i ni·ªám ph·ª©c t·∫°p m·ªôt c√°ch ƒë∆°n gi·∫£n, ƒë∆∞a ra c√°c v√≠ d·ª• th·ª±c t·∫ø v√† lu√¥n c·∫≠p nh·∫≠t c√°c xu h∆∞·ªõng m·ªõi nh·∫•t. H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát."
}
selected_persona_name = st.sidebar.selectbox("Ch·ªçn m·ªôt vai tr√≤:", options=list(personas.keys()))
system_prompt = personas[selected_persona_name]

# --- KH·ªûI T·∫†O STATE ---
if "conversation" not in st.session_state:
    st.session_state.conversation = []
if "audio_buffer" not in st.session_state:
    st.session_state.audio_buffer = None
if "last_response_audio" not in st.session_state:
    st.session_state.last_response_audio = None

# --- GIAO DI·ªÜN CH√çNH ---

# Hi·ªÉn th·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán
for entry in st.session_state.conversation:
    with st.chat_message(entry["role"]):
        st.write(entry["content"])

# T·ª± ƒë·ªông ph√°t √¢m thanh n·∫øu c√≥
if st.session_state.last_response_audio:
    st.audio(st.session_state.last_response_audio, autoplay=True)
    if os.path.exists(st.session_state.last_response_audio):
        os.remove(st.session_state.last_response_audio)
    st.session_state.last_response_audio = None

# Component ghi √¢m
webrtc_ctx = webrtc_streamer(
    key="recorder",
    mode=WebRtcMode.SENDONLY,
    audio_processor_factory=AudioRecorder,
    media_stream_constraints={"video": False, "audio": True},
)

st.write("---")
# N√∫t b·∫•m ƒë∆∞·ª£c ƒë·∫∑t b√™n ngo√†i component
if not webrtc_ctx.state.playing:
    st.info("Nh·∫•n 'Start' tr√™n khung ƒëen ·ªü tr√™n ƒë·ªÉ c·∫•p quy·ªÅn micro v√† b·∫Øt ƒë·∫ßu.")
else:
    if st.button("D·ª´ng l·∫°i v√† G·ª≠i c√¢u h·ªèi"):
        if webrtc_ctx.audio_processor:
            # L·∫•y d·ªØ li·ªáu √¢m thanh t·ª´ buffer c·ªßa audio_processor
            st.session_state.audio_buffer = webrtc_ctx.audio_processor.frames_buffer.copy()
            webrtc_ctx.audio_processor.frames_buffer.clear() # D·ªçn d·∫πp buffer
        
        # T·∫£i l·∫°i trang ƒë·ªÉ b·∫Øt ƒë·∫ßu x·ª≠ l√Ω
        st.rerun()

# --- LU·ªíNG X·ª¨ L√ù √ÇM THANH ---
# Ch·ªâ x·ª≠ l√Ω khi c√≥ d·ªØ li·ªáu trong buffer
if st.session_state.audio_buffer:
    with st.spinner("ƒêang x·ª≠ l√Ω √¢m thanh..."):
        audio_frames = st.session_state.audio_buffer
        st.session_state.audio_buffer = None # X√≥a buffer sau khi l·∫•y

        # 1. TAI: Chuy·ªÉn gi·ªçng n√≥i th√†nh vƒÉn b·∫£n
        sample_rate = 48000
        user_text = speech_to_text(audio_frames, sample_rate)

        if user_text:
            st.session_state.conversation.append({"role": "user", "content": user_text})
            
            # 2. N√ÉO: L·∫•y c√¢u tr·∫£ l·ªùi
            ai_response_text = get_ai_response(user_text, st.session_state.conversation, system_prompt)
            st.session_state.conversation.append({"role": "assistant", "content": ai_response_text})

            # 3. MI·ªÜNG: T·∫°o file √¢m thanh
            audio_file = text_to_speech(ai_response_text)
            if audio_file:
                st.session_state.last_response_audio = audio_file
            
            # T·∫£i l·∫°i l·∫ßn cu·ªëi ƒë·ªÉ hi·ªÉn th·ªã k·∫øt qu·∫£ v√† ph√°t √¢m thanh
            st.rerun()
        else:
            st.error("Kh√¥ng th·ªÉ nh·∫≠n di·ªán gi·ªçng n√≥i. Vui l√≤ng th·ª≠ l·∫°i.")
