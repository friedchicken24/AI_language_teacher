
import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase
import google.generativeai as genai
import assemblyai as aai
from google.oauth2 import service_account
import json
import os
from gtts import gTTS
import av
import numpy as np
import time
import uuid


# --- C·∫§U H√åNH BAN ƒê·∫¶U ---
st.set_page_config(page_title="Tr·ª£ l√Ω ·∫£o", page_icon="ü§ñ")
st.title("ü§ñ Tr·ª£ L√Ω ·∫¢o Th√¥ng Minh")
st.write("N√≥i chuy·ªán v·ªõi t√¥i nh√©! T√¥i ƒëang l·∫Øng nghe...")



# L·∫•y API key
try:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"]) 
except KeyError:
    st.error("‚ö†Ô∏è Vui l√≤ng th√™m GOOGLE_API_KEY v√†o Secrets c·ªßa ·ª©ng d·ª•ng.")
    st.stop()
    

# --- L·ªöP X·ª¨ L√ù √ÇM THANH ---
class AudioRecorder(AudioProcessorBase):
    def __init__(self):
        self._frames = []
        self.start_recording = False
    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        if self.start_recording:
            self._frames.append(frame.to_ndarray())
        return frame
    @property
    def frames(self):
        return self._frames
    def clear_frames(self):
        self._frames = []

# C·∫•u h√¨nh AssemblyAI
try:
    aai.settings.api_key = st.secrets["ASSEMBLYAI_API_KEY"]
except KeyError:
    st.error("‚ö†Ô∏è Vui l√≤ng th√™m ASSEMBLYAI_API_KEY v√†o Secrets.")
    st.stop()

# --- C√ÅC H√ÄM TI·ªÜN √çCH ---
def text_to_speech(text, lang='vi'):
    try:
        tts = gTTS(text=text, lang=lang, slow=False)
        # T·∫°o t√™n file duy nh·∫•t ƒë·ªÉ tr√°nh xung ƒë·ªôt
        filename = f"response_{uuid.uuid4()}.mp3"
        tts.save(filename)
        return filename
    except Exception as e:
        st.error(f"L·ªói khi t·∫°o gi·ªçng n√≥i: {e}")
        return None

def speech_to_text(audio_data, sample_rate):
    try:
        # L∆∞u file wav t·∫°m th·ªùi
        output_filename = f"input_{uuid.uuid4()}.wav"
        sound_chunk = np.concatenate(audio_data, axis=1)
        # Vi·∫øt header cho file WAV
        with open(output_filename, "wb") as f:
            f.write(b"RIFF")
            f.write(b"\x00\x00\x00\x00")
            f.write(b"WAVE")
            f.write(b"fmt ")
            f.write(b"\x10\x00\x00\x00")
            f.write(b"\x01\x00\x01\x00")
            f.write(sample_rate.to_bytes(4, "little"))
            f.write((sample_rate * 2).to_bytes(4, "little"))
            f.write(b"\x02\x00\x10\x00")
            f.write(b"data")
            f.write(len(sound_chunk.tobytes()).to_bytes(4, "little"))
            f.write(sound_chunk.tobytes())

        # G·ª≠i ƒë·∫øn AssemblyAI
        transcriber = aai.Transcriber()
        transcript = transcriber.transcribe(output_filename)

        os.remove(output_filename) # X√≥a file t·∫°m

        if transcript.status == aai.TranscriptStatus.error:
            st.error(f"L·ªói t·ª´ AssemblyAI: {transcript.error}")
            return None
        else:
            return transcript.text
    except Exception as e:
        st.error(f"L·ªói khi nh·∫≠n di·ªán gi·ªçng n√≥i qua AssemblyAI: {e}")
        return None
        
def get_ai_response(user_text, conversation_history):
    # Kh·ªüi t·∫°o model Gemini
    model = genai.GenerativeModel('gemini-1.5-flash-latest') 
    
    gemini_history = []
    for entry in conversation_history:
        # Gemini d√πng 'model' cho vai tr√≤ c·ªßa AI
        role = 'model' if entry['role'] == 'assistant' else entry['role']
        gemini_history.append({'role': role, 'parts': [entry['content']]})

    # B·∫Øt ƒë·∫ßu m·ªôt phi√™n chat v·ªõi l·ªãch s·ª≠ c≈©
    chat = model.start_chat(history=gemini_history)
    
    try:
        # G·ª≠i tin nh·∫Øn m·ªõi c·ªßa ng∆∞·ªùi d√πng
        response = chat.send_message(user_text)
        return response.text
    except Exception as e:
        st.error(f"L·ªói khi g·ªçi Gemini: {e}")
        return "T√¥i xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë v·ªõi b·ªô n√£o c·ªßa m√¨nh."

# KH·ªûI T·∫†O STATE 
if "conversation" not in st.session_state:
    st.session_state.conversation = []
if "is_recording" not in st.session_state:
    st.session_state.is_recording = False
if "audio_frames" not in st.session_state:
    st.session_state.audio_frames = []

# l·ªãch s·ª≠ tr√≤ chuy·ªán
for entry in st.session_state.conversation:
    with st.chat_message(entry["role"]):
        st.write(entry["content"])

if "last_response_audio" not in st.session_state:
    st.session_state.last_response_audio = None

if st.session_state.last_response_audio:
    st.audio(st.session_state.last_response_audio, autoplay=True)
    # X√≥a file sau khi ƒë√£ th√™m v√†o widget ƒë·ªÉ tr√°nh ƒë·∫ßy b·ªô nh·ªõ server
    if os.path.exists(st.session_state.last_response_audio):
        os.remove(st.session_state.last_response_audio)
    # Reset state ƒë·ªÉ kh√¥ng ph√°t l·∫°i ·ªü l·∫ßn rerun ti·∫øp theo
    st.session_state.last_response_audio = None

# B·ªò ƒêI·ªÄU KHI·ªÇN GHI √ÇM
# S·ª≠ d·ª•ng c·ªôt ƒë·ªÉ s·∫Øp x·∫øp c√°c n√∫t
col1, col2 = st.columns(2)
if "process_audio" not in st.session_state:
    st.session_state.process_audio = False
with col1:
    if not st.session_state.is_recording:
        if st.button("üé§ B·∫Øt ƒë·∫ßu n√≥i"):
            st.session_state.is_recording = True
            st.rerun()
     if st.button("üõë D·ª´ng l·∫°i v√† G·ª≠i"):
            st.session_state.is_recording = False
            # ƒê·∫∑t c·ªù b√°o hi·ªáu r·∫±ng c·∫ßn x·ª≠ l√Ω √¢m thanh
            st.session_state.process_audio = True 

# LU·ªíNG X·ª¨ L√ù 9
if st.session_state.is_recording:
    st.info("üî¥ ƒêang nghe... (Nh·∫•n 'D·ª´ng l·∫°i' khi b·∫°n n√≥i xong)")
    webrtc_ctx = webrtc_streamer(
        key="recorder",
        mode=WebRtcMode.SENDONLY,
        audio_processor_factory=AudioRecorder,
        media_stream_constraints={"video": False, "audio": True},
    )
    if webrtc_ctx.audio_processor:
        st.session_state.audio_frames = webrtc_ctx.audio_processor.frames

else:
    # N·∫øu kh√¥ng ghi √¢m, hi·ªÉn th·ªã th√¥ng b√°o
    st.info("Nh·∫•n 'B·∫Øt ƒë·∫ßu n√≥i' ƒë·ªÉ tr√≤ chuy·ªán v·ªõi t√¥i.")


# Ch·ªâ x·ª≠ l√Ω khi ƒë√£ d·ª´ng ghi √¢m v√† c√≥ d·ªØ li·ªáu
if st.session_state.process_audio:
    if not st.session_state.audio_frames:
        st.warning("Kh√¥ng ghi √¢m ƒë∆∞·ª£c g√¨ c·∫£. Vui l√≤ng th·ª≠ l·∫°i.")
        st.session_state.process_audio = False # Reset c·ªù
    else:
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            # L·∫•y d·ªØ li·ªáu √¢m thanh ƒë√£ l∆∞u
            audio_frames_to_process = st.session_state.audio_frames.copy()
            # X√≥a d·ªØ li·ªáu c≈© trong state ngay l·∫≠p t·ª©c
            st.session_state.audio_frames = [] 
       
        
     # 1. TAI: Chuy·ªÉn gi·ªçng n√≥i th√†nh vƒÉn b·∫£n
            sample_rate = 48000
            user_text = speech_to_text(audio_frames_to_process, sample_rate)

            if user_text:
                # C·∫≠p nh·∫≠t l·ªãch s·ª≠ tr√≤ chuy·ªán v·ªõi l·ªùi c·ªßa ng∆∞·ªùi d√πng
                st.session_state.conversation.append({"role": "user", "content": user_text})
                
                # 2. N√ÉO: L·∫•y c√¢u tr·∫£ l·ªùi t·ª´ AI
                ai_response_text = get_ai_response(user_text, st.session_state.conversation, system_prompt)
                
                # C·∫≠p nh·∫≠t l·ªãch s·ª≠ tr√≤ chuy·ªán 
                st.session_state.conversation.append({"role": "assistant", "content": ai_response_text})

                # 3. MI·ªÜNG: Chuy·ªÉn c√¢u tr·∫£ l·ªùi c·ªßa AI th√†nh gi·ªçng n√≥i v√† ph√°t
                audio_file = text_to_speech(ai_response_text)
                if audio_file:
                    st.session_state.last_response_audio = audio_file # L∆∞u t√™n file ƒë·ªÉ ph√°t ·ªü l·∫ßn ch·∫°y sau

        # Reset c·ªù x·ª≠ l√Ω sau khi ƒë√£ xong
        st.session_state.process_audio = False
        # T·∫£i l·∫°i trang M·ªòT L·∫¶N DUY NH·∫§T sau khi ƒë√£ x·ª≠ l√Ω xong
        st.rerun()
