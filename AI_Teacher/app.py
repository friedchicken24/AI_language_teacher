import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import google.generativeai as genai
import assemblyai as aai
import os
from gtts import gTTS
import av
import numpy as np
import uuid
import queue # D√πng ƒë·ªÉ giao ti·∫øp gi·ªØa c√°c lu·ªìng

# --- C·∫§U H√åNH BAN ƒê·∫¶U ---
st.set_page_config(page_title="Tr·ª£ l√Ω ·∫£o", page_icon="ü§ñ")
st.title("ü§ñ Tr·ª£ L√Ω ·∫¢o Th√¥ng Minh")
st.sidebar.title("T√πy Ch·ªçn Tr·ª£ L√Ω ·∫¢o")
st.sidebar.markdown("Ch·ªçn m·ªôt 'c√° t√≠nh' cho tr·ª£ l√Ω ·∫£o c·ªßa ch√∫ng ta!")

# L·∫•y API keys
try:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    aai.settings.api_key = st.secrets["ASSEMBLYAI_API_KEY"]
except KeyError as e:
    st.error(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y API Key: {e}. Vui l√≤ng ki·ªÉm tra l·∫°i Secrets.")
    st.stop()

# --- C√ÅC H√ÄM TI·ªÜN √çCH (Kh√¥ng thay ƒë·ªïi) ---
def text_to_speech(text, lang='vi'):
    # ... (Gi·ªØ nguy√™n h√†m n√†y)
    try:
        tts = gTTS(text=text, lang=lang, slow=False)
        filename = f"response_{uuid.uuid4()}.mp3"
        tts.save(filename)
        return filename
    except Exception as e:
        st.error(f"L·ªói khi t·∫°o gi·ªçng n√≥i: {e}")
        return None

def speech_to_text(wav_bytes):
    # ... (H√†m n√†y gi·ªù nh·∫≠n bytes thay v√¨ list of frames)
    try:
        config = aai.TranscriptionConfig(language_code="vi") # Th√™m language_code
        transcriber = aai.Transcriber(config=config)
        transcript = transcriber.transcribe(wav_bytes)

        if transcript.status == aai.TranscriptStatus.error:
            st.error(f"L·ªói t·ª´ AssemblyAI: {transcript.error}")
            return None
        return transcript.text
    except Exception as e:
        st.error(f"L·ªói khi nh·∫≠n di·ªán gi·ªçng n√≥i qua AssemblyAI: {e}")
        return None

def get_ai_response(user_text, conversation_history, system_prompt):
    # ... (Gi·ªØ nguy√™n h√†m n√†y)
    model = genai.GenerativeModel('gemini-1.5-flash-latest')
    messages = [{"role": "system", "content": system_prompt}]
    gemini_history = []
    for entry in conversation_history:
        role = 'model' if entry['role'] == 'assistant' else entry['role']
        gemini_history.append({'role': role, 'parts': [entry['content']]})
    chat = model.start_chat(history=gemini_history)
    try:
        response = chat.send_message(user_text)
        return response.text
    except Exception as e:
        st.error(f"L·ªói khi g·ªçi Gemini: {e}")
        return "T√¥i xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë v·ªõi b·ªô n√£o c·ªßa m√¨nh."

# --- C√Å T√çNH C·ª¶A AI ---
personas = {
    "Tr·ª£ l√Ω th√¢n thi·ªán": "B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o t√™n l√† Zen...",
    "Nh√† s·ª≠ h·ªçc uy√™n b√°c": "B·∫°n l√† m·ªôt nh√† s·ª≠ h·ªçc uy√™n b√°c...",
    "Chuy√™n gia c√¥ng ngh·ªá": "B·∫°n l√† m·ªôt chuy√™n gia c√¥ng ngh·ªá h√†ng ƒë·∫ßu..."
}
selected_persona_name = st.sidebar.selectbox("Ch·ªçn m·ªôt vai tr√≤:", options=list(personas.keys()))
system_prompt = personas[selected_persona_name]

# --- KH·ªûI T·∫†O STATE ---
if "conversation" not in st.session_state:
    st.session_state.conversation = []
if "last_response_audio" not in st.session_state:
    st.session_state.last_response_audio = None
if "audio_bytes" not in st.session_state:
    st.session_state.audio_bytes = None

# --- GIAO DI·ªÜN CH√çNH ---
st.write("---")

# Hi·ªÉn th·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán
for entry in st.session_state.conversation:
    with st.chat_message(entry["role"]):
        st.write(entry["content"])

# T·ª± ƒë·ªông ph√°t √¢m thanh n·∫øu c√≥
if st.session_state.last_response_audio:
    st.audio(st.session_state.last_response_audio, autoplay=True)
    if os.path.exists(st.session_state.last_response_audio):
        os.remove(st.session_state.last_response_audio)
    st.session_state.last_response_audio = None

# Component ghi √¢m v√† x·ª≠ l√Ω
# S·ª≠ d·ª•ng queue ƒë·ªÉ nh·∫≠n d·ªØ li·ªáu audio t·ª´ m·ªôt lu·ªìng kh√°c
audio_frames_queue = queue.Queue()

def audio_frame_callback(frame: av.AudioFrame):
    audio_frames_queue.put(frame.to_ndarray())

st.write("Nh·∫•n 'Start' ƒë·ªÉ b·∫Øt ƒë·∫ßu ghi √¢m, 'Stop' ƒë·ªÉ g·ª≠i.")
webrtc_ctx = webrtc_streamer(
    key="speech-to-text",
    mode=WebRtcMode.SENDONLY,
    audio_frame_callback=audio_frame_callback,
    media_stream_constraints={"video": False, "audio": True},
    rtc_configuration=RTCConfiguration({"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]})
)

# N√∫t x·ª≠ l√Ω logic
if st.button("X·ª≠ l√Ω gi·ªçng n√≥i v·ª´a ghi"):
    if not webrtc_ctx.state.playing:
        st.warning("Vui l√≤ng nh·∫•n 'Start' v√† ghi √¢m tr∆∞·ªõc khi x·ª≠ l√Ω.")
    else:
        st.info("ƒêang x·ª≠ l√Ω...")
        
        # L·∫•y t·∫•t c·∫£ c√°c frame t·ª´ queue
        audio_frames = []
        while not audio_frames_queue.empty():
            audio_frames.append(audio_frames_queue.get())
        
        if not audio_frames:
            st.warning("Kh√¥ng ghi √¢m ƒë∆∞·ª£c g√¨ c·∫£. Vui l√≤ng n√≥i v√†o micro.")
        else:
            # Gh√©p c√°c frame l·∫°i v√† t·∫°o file wav
            sample_rate = 48000
            sound_chunk = np.concatenate(audio_frames, axis=1)
            
            # Chuy·ªÉn ƒë·ªïi sang ƒë·ªãnh d·∫°ng 16-bit integer
            sound_chunk = (sound_chunk * 32767).astype(np.int16)

            # T·∫°o file wav trong b·ªô nh·ªõ
            from io import BytesIO
            import wave

            wav_buffer = BytesIO()
            with wave.open(wav_buffer, "wb") as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2) # 16-bit
                wf.setframerate(sample_rate)
                wf.writeframes(sound_chunk.tobytes())
            
            st.session_state.audio_bytes = wav_buffer.getvalue()

            # T·∫£i l·∫°i trang ƒë·ªÉ b·∫Øt ƒë·∫ßu chu·ªói ph·∫£n h·ªìi
            st.rerun()

# --- LU·ªíNG PH·∫¢N H·ªíI ---
if st.session_state.audio_bytes:
    with st.spinner("AI ƒëang l·∫Øng nghe v√† suy nghƒ©..."):
        # L·∫•y v√† x√≥a d·ªØ li·ªáu audio
        audio_to_process = st.session_state.audio_bytes
        st.session_state.audio_bytes = None

        # 1. TAI: Chuy·ªÉn gi·ªçng n√≥i th√†nh vƒÉn b·∫£n
        user_text = speech_to_text(audio_to_process)
        
        if user_text:
            st.session_state.conversation.append({"role": "user", "content": user_text})
            
            # 2. N√ÉO: L·∫•y c√¢u tr·∫£ l·ªùi
            ai_response_text = get_ai_response(user_text, st.session_state.conversation, system_prompt)
            st.session_state.conversation.append({"role": "assistant", "content": ai_response_text})

            # 3. MI·ªÜNG: T·∫°o file √¢m thanh
            audio_file = text_to_speech(ai_response_text)
            if audio_file:
                st.session_state.last_response_audio = audio_file
            
            # T·∫£i l·∫°i l·∫ßn cu·ªëi ƒë·ªÉ hi·ªÉn th·ªã k·∫øt qu·∫£ v√† ph√°t √¢m thanh
            st.rerun()
        else:
            st.error("Kh√¥ng th·ªÉ nh·∫≠n di·ªán gi·ªçng n√≥i. Vui l√≤ng th·ª≠ l·∫°i.")
            st.rerun() # T·∫£i l·∫°i ƒë·ªÉ x√≥a spinner
