
import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase
import openai
import os
from gtts import gTTS
import av
import numpy as np
import time
import uuid

# --- C·∫§U H√åNH BAN ƒê·∫¶U ---
st.set_page_config(page_title="Tr·ª£ l√Ω ·∫£o", page_icon="ü§ñ")
st.title("ü§ñ Tr·ª£ L√Ω ·∫¢o Th√¥ng Minh")
st.write("N√≥i chuy·ªán v·ªõi t√¥i nh√©! T√¥i ƒëang l·∫Øng nghe...")

# L·∫•y API key
try:
    openai.api_key = st.secrets["OPENAI_API_KEY"]
except KeyError:
    st.error("‚ö†Ô∏è Vui l√≤ng th√™m OpenAI API key v√†o Secrets c·ªßa ·ª©ng d·ª•ng.")
    st.stop()

# --- L·ªöP X·ª¨ L√ù √ÇM THANH ---
class AudioRecorder(AudioProcessorBase):
    def __init__(self):
        self._frames = []
        self.start_recording = False
    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        if self.start_recording:
            self._frames.append(frame.to_ndarray())
        return frame
    @property
    def frames(self):
        return self._frames
    def clear_frames(self):
        self._frames = []

# --- C√ÅC H√ÄM TI·ªÜN √çCH ---
def text_to_speech(text, lang='vi'):
    try:
        tts = gTTS(text=text, lang=lang, slow=False)
        # T·∫°o t√™n file duy nh·∫•t ƒë·ªÉ tr√°nh xung ƒë·ªôt
        filename = f"response_{uuid.uuid4()}.mp3"
        tts.save(filename)
        return filename
    except Exception as e:
        st.error(f"L·ªói khi t·∫°o gi·ªçng n√≥i: {e}")
        return None

def speech_to_text(audio_data, sample_rate):
    try:
        # L∆∞u file wav t·∫°m th·ªùi
        output_filename = f"input_{uuid.uuid4()}.wav"
        sound_chunk = np.concatenate(audio_data, axis=1)
        # Vi·∫øt header cho file WAV
        with open(output_filename, "wb") as f:
            f.write(b"RIFF")
            f.write(b"\x00\x00\x00\x00")
            f.write(b"WAVE")
            f.write(b"fmt ")
            f.write(b"\x10\x00\x00\x00")
            f.write(b"\x01\x00\x01\x00")
            f.write(sample_rate.to_bytes(4, "little"))
            f.write((sample_rate * 2).to_bytes(4, "little"))
            f.write(b"\x02\x00\x10\x00")
            f.write(b"data")
            f.write(len(sound_chunk.tobytes()).to_bytes(4, "little"))
            f.write(sound_chunk.tobytes())
        
        # G·ª≠i ƒë·∫øn Whisper
        with open(output_filename, "rb") as audio_file:
            transcript = openai.audio.transcriptions.create(model="whisper-1", file=audio_file)
        os.remove(output_filename) # X√≥a file t·∫°m
        return transcript.text
    except Exception as e:
        st.error(f"L·ªói khi nh·∫≠n di·ªán gi·ªçng n√≥i: {e}")
        return None
        
def get_ai_response(user_text, conversation_history):
    messages = [{"role": "system", "content": "B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o th√¢n thi·ªán, th√¥ng minh v√† n√≥i chuy·ªán b·∫±ng ti·∫øng Vi·ªát. H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† t·ª± nhi√™n."}]
    messages.extend(conversation_history)
    messages.append({"role": "user", "content": user_text})
    
    try:
        response = openai.chat.completions.create(model="gpt-4o", messages=messages)
        ai_text = response.choices[0].message.content
        return ai_text
    except Exception as e:
        st.error(f"L·ªói khi g·ªçi GPT: {e}")
        return "T√¥i xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë."

# --- KH·ªûI T·∫†O STATE C·ª¶A ·ª®NG D·ª§NG ---
if "conversation" not in st.session_state:
    st.session_state.conversation = []
if "is_recording" not in st.session_state:
    st.session_state.is_recording = False
if "audio_frames" not in st.session_state:
    st.session_state.audio_frames = []

# Hi·ªÉn th·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán
for entry in st.session_state.conversation:
    with st.chat_message(entry["role"]):
        st.write(entry["content"])

# --- B·ªò ƒêI·ªÄU KHI·ªÇN GHI √ÇM ---
# S·ª≠ d·ª•ng c·ªôt ƒë·ªÉ s·∫Øp x·∫øp c√°c n√∫t
col1, col2 = st.columns(2)

with col1:
    if not st.session_state.is_recording:
        if st.button("üé§ B·∫Øt ƒë·∫ßu n√≥i"):
            st.session_state.is_recording = True
            st.rerun()
    else:
        if st.button("üõë D·ª´ng l·∫°i v√† G·ª≠i"):
            st.session_state.is_recording = False
            st.rerun()

# --- LU·ªíNG X·ª¨ L√ù CH√çNH ---
if st.session_state.is_recording:
    st.info("üî¥ ƒêang nghe... (Nh·∫•n 'D·ª´ng l·∫°i' khi b·∫°n n√≥i xong)")
    webrtc_ctx = webrtc_streamer(
        key="recorder",
        mode=WebRtcMode.SENDONLY,
        audio_processor_factory=AudioRecorder,
        media_stream_constraints={"video": False, "audio": True},
    )
    if webrtc_ctx.audio_processor:
        st.session_state.audio_frames = webrtc_ctx.audio_processor.frames

# Ch·ªâ x·ª≠ l√Ω khi ƒë√£ d·ª´ng ghi √¢m v√† c√≥ d·ªØ li·ªáu
if not st.session_state.is_recording and st.session_state.audio_frames:
    with st.spinner("ƒêang x·ª≠ l√Ω..."):
        # 1. TAI: Chuy·ªÉn gi·ªçng n√≥i th√†nh vƒÉn b·∫£n
        sample_rate = 48000 # T·∫ßn s·ªë m·∫´u m·∫∑c ƒë·ªãnh c·ªßa webrtc
        user_text = speech_to_text(st.session_state.audio_frames, sample_rate)
        st.session_state.audio_frames = [] # X√≥a d·ªØ li·ªáu c≈©

        if user_text:
            # C·∫≠p nh·∫≠t l·ªãch s·ª≠ tr√≤ chuy·ªán v·ªõi l·ªùi c·ªßa ng∆∞·ªùi d√πng
            st.session_state.conversation.append({"role": "user", "content": user_text})
            
            # 2. N√ÉO: L·∫•y c√¢u tr·∫£ l·ªùi t·ª´ AI
            ai_response_text = get_ai_response(user_text, st.session_state.conversation)
            
            # C·∫≠p nh·∫≠t l·ªãch s·ª≠ tr√≤ chuy·ªán v·ªõi l·ªùi c·ªßa AI
            st.session_state.conversation.append({"role": "assistant", "content": ai_response_text})

            # 3. MI·ªÜNG: Chuy·ªÉn c√¢u tr·∫£ l·ªùi c·ªßa AI th√†nh gi·ªçng n√≥i v√† ph√°t
            audio_file = text_to_speech(ai_response_text)
            if audio_file:
                # T·ª± ƒë·ªông ph√°t √¢m thanh v√† x√≥a file
                st.audio(audio_file, autoplay=True)
                time.sleep(1) # Ch·ªù m·ªôt ch√∫t ƒë·ªÉ ch·∫Øc ch·∫Øn st.audio ƒë√£ load
                os.remove(audio_file)
            
            # T·∫£i l·∫°i trang ƒë·ªÉ hi·ªÉn th·ªã cu·ªôc h·ªôi tho·∫°i m·ªõi
            st.rerun()
