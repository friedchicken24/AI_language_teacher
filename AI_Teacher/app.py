import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import google.generativeai as genai
import assemblyai as aai
import os
from gtts import gTTS
import av
import numpy as np
import uuid
import queue
from io import BytesIO
import wave

# --- C·∫§U H√åNH BAN ƒê·∫¶U ---
st.set_page_config(page_title="Tr·ª£ l√Ω ·∫£o", page_icon="ü§ñ", layout="wide")
st.title("ü§ñ Tr·ª£ L√Ω ·∫¢o Th√¥ng Minh")

# --- C·∫§U H√åNH API KEYS V√Ä CLIENTS ---
try:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    aai.settings.api_key = st.secrets["ASSEMBLYAI_API_KEY"]
except KeyError as e:
    st.error(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y API Key: {e}. Vui l√≤ng ki·ªÉm tra l·∫°i Secrets.")
    st.stop()

# --- C√ÅC H√ÄM TI·ªÜN √çCH (Kh√¥ng thay ƒë·ªïi) ---
def text_to_speech(text, lang='vi'):
    try:
        tts = gTTS(text=text, lang=lang, slow=False)
        filename = f"response_{uuid.uuid4()}.mp3"
        tts.save(filename)
        return filename
    except Exception as e:
        st.error(f"L·ªói khi t·∫°o gi·ªçng n√≥i: {e}")
        return None

def speech_to_text(wav_bytes):
    try:
        config = aai.TranscriptionConfig(language_code="vi")
        transcriber = aai.Transcriber(config=config)
        transcript = transcriber.transcribe(wav_bytes)

        if transcript.status == aai.TranscriptStatus.error:
            st.error(f"L·ªói t·ª´ AssemblyAI: {transcript.error}")
            return None
        return transcript.text
    except Exception as e:
        st.error(f"L·ªói khi nh·∫≠n di·ªán gi·ªçng n√≥i qua AssemblyAI: {e}")
        return None

def get_ai_response(user_text, conversation_history, system_prompt):
    model = genai.GenerativeModel('gemini-1.5-flash-latest')
    messages = [{"role": "system", "content": system_prompt}]
    gemini_history = []
    for entry in conversation_history:
        role = 'model' if entry['role'] == 'assistant' else entry['role']
        gemini_history.append({'role': role, 'parts': [entry['content']]})
    chat = model.start_chat(history=gemini_history)
    try:
        response = chat.send_message(user_text)
        return response.text
    except Exception as e:
        st.error(f"L·ªói khi g·ªçi Gemini: {e}")
        return "T√¥i xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë v·ªõi b·ªô n√£o c·ªßa m√¨nh."

# --- GIAO DI·ªÜN V√Ä LOGIC ---
with st.sidebar:
    st.title("T√πy Ch·ªçn Tr·ª£ L√Ω ·∫¢o")
    personas = {
        "Tr·ª£ l√Ω th√¢n thi·ªán": "B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o t√™n l√† Zen...",
        "Nh√† s·ª≠ h·ªçc uy√™n b√°c": "B·∫°n l√† m·ªôt nh√† s·ª≠ h·ªçc uy√™n b√°c...",
        "Chuy√™n gia c√¥ng ngh·ªá": "B·∫°n l√† m·ªôt chuy√™n gia c√¥ng ngh·ªá h√†ng ƒë·∫ßu..."
    }
    selected_persona_name = st.selectbox("Ch·ªçn m·ªôt vai tr√≤:", options=list(personas.keys()))
    system_prompt = personas[selected_persona_name]

# Kh·ªüi t·∫°o session state
if "conversation" not in st.session_state:
    st.session_state.conversation = []
if "last_response_audio" not in st.session_state:
    st.session_state.last_response_audio = None
if "audio_buffer" not in st.session_state:
    st.session_state.audio_buffer = queue.Queue()

# Hi·ªÉn th·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán
chat_container = st.container()
with chat_container:
    for entry in st.session_state.conversation:
        with st.chat_message(entry["role"]):
            st.write(entry["content"])

# T·ª± ƒë·ªông ph√°t √¢m thanh
if st.session_state.last_response_audio:
    st.audio(st.session_state.last_response_audio, autoplay=True)
    if os.path.exists(st.session_state.last_response_audio):
        os.remove(st.session_state.last_response_audio)
    st.session_state.last_response_audio = None

# Component ghi √¢m
st.write("---")
col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("B·∫£ng ƒëi·ªÅu khi·ªÉn")
    # S·ª≠ d·ª•ng callback ƒë·ªÉ ghi √¢m
    def audio_frame_callback(frame: av.AudioFrame):
        st.session_state.audio_buffer.put(frame.to_ndarray())

    webrtc_ctx = webrtc_streamer(
        key="recorder",
        mode=WebRtcMode.SENDONLY,
        audio_frame_callback=audio_frame_callback,
        media_stream_constraints={"video": False, "audio": True},
        rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
    )

with col2:
    st.subheader("Tr·∫°ng th√°i")
    if webrtc_ctx.state.playing:
        st.success("üî¥ Micro ƒëang b·∫≠t. H√£y n√≥i ƒëi!")
        if st.button("D·ª´ng v√† g·ª≠i"):
            # L·∫•y d·ªØ li·ªáu t·ª´ queue
            frames = []
            while not st.session_state.audio_buffer.empty():
                frames.append(st.session_state.audio_buffer.get())

            if not frames:
                st.warning("Kh√¥ng c√≥ √¢m thanh n√†o ƒë∆∞·ª£c ghi l·∫°i. Vui l√≤ng n√≥i g·∫ßn micro h∆°n.")
            else:
                st.info("ƒê√£ nh·∫≠n ƒë∆∞·ª£c √¢m thanh. ƒêang x·ª≠ l√Ω...")
                
                # Gh√©p c√°c frame v√† t·∫°o file wav trong b·ªô nh·ªõ
                sound_chunk = np.concatenate(frames, axis=1)
                sound_chunk = (sound_chunk * 32767).astype(np.int16)
                
                wav_buffer = BytesIO()
                with wave.open(wav_buffer, "wb") as wf:
                    wf.setnchannels(1)
                    wf.setsampwidth(2)
                    wf.setframerate(48000)
                    wf.writeframes(sound_chunk.tobytes())
                
                wav_bytes = wav_buffer.getvalue()

                # ---- B·∫ÆT ƒê·∫¶U LU·ªíNG X·ª¨ L√ù ----
                with st.spinner("AI ƒëang l·∫Øng nghe..."):
                    user_text = speech_to_text(wav_bytes)

                if user_text:
                    st.session_state.conversation.append({"role": "user", "content": user_text})
                    with st.spinner("AI ƒëang suy nghƒ©..."):
                        ai_response_text = get_ai_response(user_text, st.session_state.conversation, system_prompt)
                    
                    st.session_state.conversation.append({"role": "assistant", "content": ai_response_text})
                    
                    with st.spinner("AI ƒëang chu·∫©n b·ªã n√≥i..."):
                        audio_file = text_to_speech(ai_response_text)
                    
                    if audio_file:
                        st.session_state.last_response_audio = audio_file
                    
                    st.rerun()
                else:
                    st.error("Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c gi·ªçng n√≥i. H√£y th·ª≠ n√≥i to v√† r√µ h∆°n.")
    else:
        st.info("Nh·∫•n 'Start' tr√™n khung ƒëen ƒë·ªÉ c·∫•p quy·ªÅn v√† b·∫Øt ƒë·∫ßu ghi √¢m.")
