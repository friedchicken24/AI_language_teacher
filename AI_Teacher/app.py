
import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase
import google.generativeai as genai
import os
from gtts import gTTS
import av
import numpy as np
import time
import uuid

# --- C·∫§U H√åNH BAN ƒê·∫¶U ---
st.set_page_config(page_title="Tr·ª£ l√Ω ·∫£o", page_icon="ü§ñ")
st.title("ü§ñ Tr·ª£ L√Ω ·∫¢o Th√¥ng Minh")
st.write("N√≥i chuy·ªán v·ªõi t√¥i nh√©! T√¥i ƒëang l·∫Øng nghe...")

# L·∫•y API key
try:
    # Thay ƒë·ªïi t√™n secret ƒë·ªÉ ph√π h·ª£p v·ªõi Google
    genai.configure(api_key=st.secrets["AIzaSyBK0odFXfU4KyBqaqFV9ioV15_5pjC3_3k"])
except KeyError:
    st.error("‚ö†Ô∏è Vui l√≤ng th√™m GOOGLE_API_KEY v√†o Secrets c·ªßa ·ª©ng d·ª•ng.")
    st.stop()

# --- L·ªöP X·ª¨ L√ù √ÇM THANH ---
class AudioRecorder(AudioProcessorBase):
    def __init__(self):
        self._frames = []
        self.start_recording = False
    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        if self.start_recording:
            self._frames.append(frame.to_ndarray())
        return frame
    @property
    def frames(self):
        return self._frames
    def clear_frames(self):
        self._frames = []

# --- C√ÅC H√ÄM TI·ªÜN √çCH ---
def text_to_speech(text, lang='vi'):
    try:
        tts = gTTS(text=text, lang=lang, slow=False)
        # T·∫°o t√™n file duy nh·∫•t ƒë·ªÉ tr√°nh xung ƒë·ªôt
        filename = f"response_{uuid.uuid4()}.mp3"
        tts.save(filename)
        return filename
    except Exception as e:
        st.error(f"L·ªói khi t·∫°o gi·ªçng n√≥i: {e}")
        return None

def speech_to_text(audio_data, sample_rate):
    try:
        # L∆∞u file wav t·∫°m th·ªùi
        output_filename = f"input_{uuid.uuid4()}.wav"
        sound_chunk = np.concatenate(audio_data, axis=1)
        # Vi·∫øt header cho file WAV
        with open(output_filename, "wb") as f:
            f.write(b"RIFF")
            f.write(b"\x00\x00\x00\x00")
            f.write(b"WAVE")
            f.write(b"fmt ")
            f.write(b"\x10\x00\x00\x00")
            f.write(b"\x01\x00\x01\x00")
            f.write(sample_rate.to_bytes(4, "little"))
            f.write((sample_rate * 2).to_bytes(4, "little"))
            f.write(b"\x02\x00\x10\x00")
            f.write(b"data")
            f.write(len(sound_chunk.tobytes()).to_bytes(4, "little"))
            f.write(sound_chunk.tobytes())
        
        # G·ª≠i ƒë·∫øn Whisper
        with open(output_filename, "rb") as audio_file:
            transcript = openai.audio.transcriptions.create(model="whisper-1", file=audio_file)
        os.remove(output_filename) # X√≥a file t·∫°m
        return transcript.text
    except Exception as e:
        st.error(f"L·ªói khi nh·∫≠n di·ªán gi·ªçng n√≥i: {e}")
        return None
        
def get_ai_response(user_text, conversation_history):
    # Kh·ªüi t·∫°o model Gemini
    model = genai.GenerativeModel('gemini-1.5-flash-latest') # D√πng b·∫£n Flash cho t·ªëc ƒë·ªô nhanh
    
    gemini_history = []
    for entry in conversation_history:
        # Gemini d√πng 'model' cho vai tr√≤ c·ªßa AI
        role = 'model' if entry['role'] == 'assistant' else entry['role']
        gemini_history.append({'role': role, 'parts': [entry['content']]})

    # B·∫Øt ƒë·∫ßu m·ªôt phi√™n chat v·ªõi l·ªãch s·ª≠ c≈©
    chat = model.start_chat(history=gemini_history)
    
    try:
        # G·ª≠i tin nh·∫Øn m·ªõi c·ªßa ng∆∞·ªùi d√πng
        response = chat.send_message(user_text)
        return response.text
    except Exception as e:
        st.error(f"L·ªói khi g·ªçi Gemini: {e}")
        return "T√¥i xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë v·ªõi b·ªô n√£o c·ªßa m√¨nh."

# --- KH·ªûI T·∫†O STATE C·ª¶A ·ª®NG D·ª§NG ---
if "conversation" not in st.session_state:
    st.session_state.conversation = []
if "is_recording" not in st.session_state:
    st.session_state.is_recording = False
if "audio_frames" not in st.session_state:
    st.session_state.audio_frames = []

# Hi·ªÉn th·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán
for entry in st.session_state.conversation:
    with st.chat_message(entry["role"]):
        st.write(entry["content"])

# --- B·ªò ƒêI·ªÄU KHI·ªÇN GHI √ÇM ---
# S·ª≠ d·ª•ng c·ªôt ƒë·ªÉ s·∫Øp x·∫øp c√°c n√∫t
col1, col2 = st.columns(2)

with col1:
    if not st.session_state.is_recording:
        if st.button("üé§ B·∫Øt ƒë·∫ßu n√≥i"):
            st.session_state.is_recording = True
            st.rerun()
    else:
        if st.button("üõë D·ª´ng l·∫°i v√† G·ª≠i"):
            st.session_state.is_recording = False
            st.rerun()

# --- LU·ªíNG X·ª¨ L√ù CH√çNH ---
if st.session_state.is_recording:
    st.info("üî¥ ƒêang nghe... (Nh·∫•n 'D·ª´ng l·∫°i' khi b·∫°n n√≥i xong)")
    webrtc_ctx = webrtc_streamer(
        key="recorder",
        mode=WebRtcMode.SENDONLY,
        audio_processor_factory=AudioRecorder,
        media_stream_constraints={"video": False, "audio": True},
    )
    if webrtc_ctx.audio_processor:
        st.session_state.audio_frames = webrtc_ctx.audio_processor.frames

# Ch·ªâ x·ª≠ l√Ω khi ƒë√£ d·ª´ng ghi √¢m v√† c√≥ d·ªØ li·ªáu
if not st.session_state.is_recording and st.session_state.audio_frames:
    with st.spinner("ƒêang x·ª≠ l√Ω..."):
        # 1. TAI: Chuy·ªÉn gi·ªçng n√≥i th√†nh vƒÉn b·∫£n
        sample_rate = 48000 # T·∫ßn s·ªë m·∫´u m·∫∑c ƒë·ªãnh c·ªßa webrtc
        user_text = speech_to_text(st.session_state.audio_frames, sample_rate)
        st.session_state.audio_frames = [] # X√≥a d·ªØ li·ªáu c≈©

        if user_text:
            # C·∫≠p nh·∫≠t l·ªãch s·ª≠ tr√≤ chuy·ªán v·ªõi l·ªùi c·ªßa ng∆∞·ªùi d√πng
            st.session_state.conversation.append({"role": "user", "content": user_text})
            
            # 2. N√ÉO: L·∫•y c√¢u tr·∫£ l·ªùi t·ª´ AI
            ai_response_text = get_ai_response(user_text, st.session_state.conversation)
            
            # C·∫≠p nh·∫≠t l·ªãch s·ª≠ tr√≤ chuy·ªán v·ªõi l·ªùi c·ªßa AI
            st.session_state.conversation.append({"role": "assistant", "content": ai_response_text})

            # 3. MI·ªÜNG: Chuy·ªÉn c√¢u tr·∫£ l·ªùi c·ªßa AI th√†nh gi·ªçng n√≥i v√† ph√°t
            audio_file = text_to_speech(ai_response_text)
            if audio_file:
                # T·ª± ƒë·ªông ph√°t √¢m thanh v√† x√≥a file
                st.audio(audio_file, autoplay=True)
                time.sleep(1) # Ch·ªù m·ªôt ch√∫t ƒë·ªÉ ch·∫Øc ch·∫Øn st.audio ƒë√£ load
                os.remove(audio_file)
            
            # T·∫£i l·∫°i trang ƒë·ªÉ hi·ªÉn th·ªã cu·ªôc h·ªôi tho·∫°i m·ªõi
            st.rerun()
